{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "# use pandas to import data as data frame\n",
    "dataset = pd.read_csv('../Data Preprocessing Template/Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data fram split data into x (independent), y (dependent) arrays\n",
    "# all machine learning libs work with arrays (not with dataframes) \n",
    "x = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling missing values\n",
    "For machine learning analyses missing values are usually handled by imputing some common value like mean, median or modal. This is done because by removing records that have missing values, especially when lot of variables are used during analyses will significantly reduce sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values with mean value of the column\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "imputer.fit(x[:, 1:3])\n",
    "x[:, 1:3] = imputer.transform(x[:, 1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding categorical data\n",
    "Categorical date expressed as text values need to prepared for the analyses. The preparation is twofold:\n",
    "\n",
    "- Encoding strings to numbers: First action is to convert string values into numbers as machine learning alogs work only with numbers\n",
    "- Removing numerical meaning from categorical value: in case when we have 'pure' categorical scale/value where position/number does not have meaning, we need to recode categorical values into boolean type (true/false). In this example, countries are records to numbers in the first instance and because values of each countries are not orignal or ration scale we convert it to array having 0/1 (true/false) value for each of country values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import encoders from scikit\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode country values into boolean array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init label encoder\n",
    "country_encoder = LabelEncoder()\n",
    "# convert labels of first column (country) to numerical values\n",
    "x[:,0] = country_encoder.fit_transform(x[:,0])\n",
    "# init boolean encoder and convert values into array of \n",
    "# categorical_features param takes col to transform from values into boolean array, \n",
    "# here is this col country \n",
    "bool_encoder = OneHotEncoder(categorical_features = [0])\n",
    "x = bool_encoder.fit_transform(x).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode dependent (y) purchase y/n into numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no = 0, yes = 1\n",
    "purchase_encoder = LabelEncoder()\n",
    "y = purchase_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting data set into training and test sets\n",
    "Total data is usually split into 2 parts. In this example we use cross_validation module from scikit learn. In this example we use train_test_split function from scikit_learn. We pass arrays of independent, dependent var and test_size as value between 0 - 1 (representing % of split). Common split is 80/20, where 80% is used for training. Split is performed at random, but for replication one should provide random_state value.\n",
    "\n",
    "`NOTE! cross_validation module is replaced with model_selection since v0.18`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cross validation module from scikit-learn\n",
    "# NOTE! \n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature scaling / 'normalizing' variable values\n",
    "\n",
    "Machine learning analyses are based on Eucledian distance, eg. distance between values has meaning. When different variables use different scales this has influence on calculation. Therefore during data preparation variables are 'normalized'. Generally there are two ways of 'removing' influence of different variable scales:\n",
    "\n",
    "- standarisation: values are standardised by deducting actual value from the mean value and divinding it by standard deviation. x = x - mean(x) / std.dev (x)\n",
    "- normalizaration: bringing values between -1 and 1 by taking difference between actual value and minimal value and divinding it by value range, x = x - min(x) / max(x) - min(x)\n",
    "\n",
    "For normalization in this example we use sckit-learn module StandardScaler. The same 'scaler' is used on train and test data, where fit & transform is only used on first set (train_x in this example). The dependent variable is in this case not 'normilized' because it is an simple classification problem with 2 categories. In the regression situation when dependent variable has wide range of values normalization should also be applied to dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scale_x = StandardScaler()\n",
    "# for training set we need to fit and transform\n",
    "x_train = scale_x.fit_transform(x_train)\n",
    "# for test set we only transform because \n",
    "# the scale values were already fitted in first run\n",
    "# notice that we use same scale_x object \n",
    "x_test = scale_x.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.645751</td>\n",
       "      <td>-0.774597</td>\n",
       "      <td>0.263068</td>\n",
       "      <td>0.123815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>-0.774597</td>\n",
       "      <td>-0.253501</td>\n",
       "      <td>0.461756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>1.290994</td>\n",
       "      <td>-1.975398</td>\n",
       "      <td>-1.530933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>1.290994</td>\n",
       "      <td>0.052614</td>\n",
       "      <td>-1.111420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>-0.774597</td>\n",
       "      <td>1.640585</td>\n",
       "      <td>1.720297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>1.290994</td>\n",
       "      <td>-0.081312</td>\n",
       "      <td>-0.167514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>-0.774597</td>\n",
       "      <td>0.951826</td>\n",
       "      <td>0.986148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>-0.774597</td>\n",
       "      <td>-0.597881</td>\n",
       "      <td>-0.482149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0         1         2         3         4\n",
       "0 -1.0  2.645751 -0.774597  0.263068  0.123815\n",
       "1  1.0 -0.377964 -0.774597 -0.253501  0.461756\n",
       "2 -1.0 -0.377964  1.290994 -1.975398 -1.530933\n",
       "3 -1.0 -0.377964  1.290994  0.052614 -1.111420\n",
       "4  1.0 -0.377964 -0.774597  1.640585  1.720297\n",
       "5 -1.0 -0.377964  1.290994 -0.081312 -0.167514\n",
       "6  1.0 -0.377964 -0.774597  0.951826  0.986148\n",
       "7  1.0 -0.377964 -0.774597 -0.597881 -0.482149"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframe from x_train\n",
    "# just to be able to print it here as table\n",
    "# for visual controle\n",
    "test = pd.DataFrame(x_train)\n",
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
